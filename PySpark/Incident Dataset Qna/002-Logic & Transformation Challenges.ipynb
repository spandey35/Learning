{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67381791-4912-4fbc-8a12-25fdb3a67977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/surajss.3110@gmail.com/Learning/PySpark/MasterNoteBooks/INC_MasterNoteBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23371f7-78e3-4c91-8467-27506d459af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inc = load_INC()\n",
    "display(inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39916c3d-8228-49bd-a0b2-0adbcfb3dba5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Priority\":159},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754204439867}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Dynamic SLA Breach Classification Create a logic that adjusts SLA breach thresholds based on ticket priority and business hours.\n",
    "\n",
    "def SLAThresholds(Priority,Resolved_Timing_In_Hours):\n",
    "    thresholds = {\n",
    "        \"1-Critical\": 1,\n",
    "        \"2-High\": 4,\n",
    "        \"Medium\": 16,\n",
    "        \"Low\": 24\n",
    "    }\n",
    "    limit = thresholds.get(Priority, 24)  # default to 24 if unknown\n",
    "    return \"SLA Breached\" if Resolved_Timing_In_Hours > limit else \"SLA Not Breached\"\n",
    " \n",
    "def is_weekend(timestamp):\n",
    "    day = timestamp.weekday()  # Monday = 0, Sunday = 6\n",
    "    return \"Weekend\" if day >= 5 else \"Weekday\"\n",
    "\n",
    "udf1= udf(SLAThresholds,StringType())    \n",
    "udf2= udf(is_weekend,StringType())    \n",
    "\n",
    "df2a = (inc\n",
    "        .withColumn(\"Opened\",expr(\"try_to_timestamp(Opened,'MM/dd/yyyy HH:mm:ss')\"))\n",
    "        .withColumn(\"Resolved\",expr(\"try_to_timestamp(Resolved,'MM/dd/yyyy HH:mm:ss')\"))\n",
    "        .na.drop(subset=[\"Opened\",\"Resolved\"])\n",
    "        .withColumn(\"Resolved_Timing_in_Hours\",(unix_timestamp(\"Resolved\") - unix_timestamp(\"Opened\"))/ 3600)\n",
    "        .withColumn(\"Priority\",udf1(\"Priority\",\"Resolved_Timing_in_Hours\"))\n",
    "        .withColumn(\"Resolved_DayType\", udf2(\"Resolved\"))\n",
    "        .select(\"Issue\",\"Opened\",\"Resolved\",\"Resolved_Timing_in_Hours\",\"Resolved_DayType\",\"Priority\")\n",
    "        )\n",
    "display(df2a)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adef515c-2757-45f7-b31f-4cc248708370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView115362b\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView115362b\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView115362b\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView115362b) SELECT `Priority`,`SLA_Status`,SUM(`Resolved_Timing_in_Hours`) `Resolved_Timing_in_Hours_alias` FROM q GROUP BY `Priority`,`SLA_Status` ORDER BY `Priority` ASC,`SLA_Status` ASC,`SLA_Status` ASC\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView115362b\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "PIVOT_TABLE"
         },
         {
          "key": "options",
          "value": {
           "cell": {
            "field": "Resolved_Timing_in_Hours",
            "transform": {
             "fn": "sum"
            }
           },
           "colorCellsByValue": false,
           "columns": [
            {
             "field": "SLA_Status",
             "sort": "ascending"
            }
           ],
           "rows": [
            {
             "field": "Priority",
             "sort": "ascending"
            },
            {
             "field": "SLA_Status",
             "sort": "ascending"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "c4eb766e-5bc5-44c4-ae86-051bdcffb888",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Priority",
           "type": "column"
          },
          {
           "column": "SLA_Status",
           "type": "column"
          }
         ],
         "order_by": [
          {
           "direction": "asc",
           "expression": {
            "identifier": "Priority",
            "kind": "identifier"
           }
          },
          {
           "direction": "asc",
           "expression": {
            "identifier": "SLA_Status",
            "kind": "identifier"
           }
          },
          {
           "direction": "asc",
           "expression": {
            "identifier": "SLA_Status",
            "kind": "identifier"
           }
          }
         ],
         "selects": [
          {
           "column": "Priority",
           "type": "column"
          },
          {
           "column": "SLA_Status",
           "type": "column"
          },
          {
           "alias": "Resolved_Timing_in_Hours_alias",
           "args": [
            {
             "column": "Resolved_Timing_in_Hours",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IHVkZiwgZXhwciwgdW5peF90aW1lc3RhbXAKZnJvbSBweXNwYXJrLnNxbC50eXBlcyBpbXBvcnQgU3RyaW5nVHlwZQpmcm9tIGRhdGV0aW1lIGltcG9ydCBkYXRldGltZSwgdGltZWRlbHRhCgojIDEuIERlZmluZSB0aGUgU0xBIGNsYXNzaWZpY2F0aW9uIGZ1bmN0aW9uCmRlZiBjbGFzc2lmeV9zbGEocHJpb3JpdHksIG9wZW5lZCwgcmVzb2x2ZWQpOgogICAgaWYgbm90IG9wZW5lZCBvciBub3QgcmVzb2x2ZWQ6CiAgICAgICAgcmV0dXJuICJVbmtub3duIgoKICAgICMgQ29udmVydCB0byBkYXRldGltZSBpZiBuZWVkZWQKICAgIGlmIGlzaW5zdGFuY2Uob3BlbmVkLCBzdHIpOgogICAgICAgIG9wZW5lZCA9IGRhdGV0aW1lLnN0cnB0aW1lKG9wZW5lZCwgIiVZLSVtLSVkICVIOiVNOiVTIikKICAgIGlmIGlzaW5zdGFuY2UocmVzb2x2ZWQsIHN0cik6CiAgICAgICAgcmVzb2x2ZWQgPSBkYXRldGltZS5zdHJwdGltZShyZXNvbHZlZCwgIiVZLSVtLSVkICVIOiVNOiVTIikKCiAgICAjIENhbGN1bGF0ZSBidXNpbmVzcyBob3VycyAoZXhjbHVkaW5nIHdlZWtlbmRzKQogICAgY3VycmVudCA9IG9wZW5lZAogICAgdG90YWxfaG91cnMgPSAwCiAgICB3aGlsZSBjdXJyZW50IDwgcmVzb2x2ZWQ6CiAgICAgICAgaWYgY3VycmVudC53ZWVrZGF5KCkgPCA1OiAgIyBNb25kYXkgdG8gRnJpZGF5CiAgICAgICAgICAgIHRvdGFsX2hvdXJzICs9IDEKICAgICAgICBjdXJyZW50ICs9IHRpbWVkZWx0YShob3Vycz0xKQoKICAgICMgQXBwbHkgU0xBIHRocmVzaG9sZHMKICAgIGlmIHByaW9yaXR5ID09ICIxLUNyaXRpY2FsIjoKICAgICAgICByZXR1cm4gIlNMQSBCcmVhY2hlZCIgaWYgdG90YWxfaG91cnMgPiAxIGVsc2UgIlNMQSBOb3QgQnJlYWNoZWQiCiAgICBlbGlmIHByaW9yaXR5ID09ICIyLUhpZ2giOgogICAgICAgIHJldHVybiAiU0xBIEJyZWFjaGVkIiBpZiB0b3RhbF9ob3VycyA+IDYgZWxzZSAiU0xBIE5vdCBCcmVhY2hlZCIKICAgIGVsaWYgcHJpb3JpdHkgPT0gIk1lZGl1bSI6CiAgICAgICAgcmV0dXJuICJTTEEgQnJlYWNoZWQiIGlmIHRvdGFsX2hvdXJzID4gNDggZWxzZSAiU0xBIE5vdCBCcmVhY2hlZCIgICMgMiBidXNpbmVzcyBkYXlzCiAgICBlbGlmIHByaW9yaXR5ID09ICJMb3ciOgogICAgICAgIHJldHVybiAiU0xBIEJyZWFjaGVkIiBpZiB0b3RhbF9ob3VycyA+IDk2IGVsc2UgIlNMQSBOb3QgQnJlYWNoZWQiICAjIDQgYnVzaW5lc3MgZGF5cwogICAgZWxzZToKICAgICAgICByZXR1cm4gIlVua25vd24iCgojIDIuIFJlZ2lzdGVyIHRoZSBmdW5jdGlvbiBhcyBhIFVERgpzbGFfdWRmID0gdWRmKGNsYXNzaWZ5X3NsYSwgU3RyaW5nVHlwZSgpKQoKIyAzLiBBcHBseSB0byB5b3VyIERhdGFGcmFtZQpkZl9zbGEgPSAoaW5jCiAgICAud2l0aENvbHVtbigiT3BlbmVkIiwgZXhwcigidHJ5X3RvX3RpbWVzdGFtcChPcGVuZWQsJ01NL2RkL3l5eXkgSEg6bW06c3MnKSIpKQogICAgLndpdGhDb2x1bW4oIlJlc29sdmVkIiwgZXhwcigidHJ5X3RvX3RpbWVzdGFtcChSZXNvbHZlZCwnTU0vZGQveXl5eSBISDptbTpzcycpIikpCiAgICAubmEuZHJvcChzdWJzZXQ9WyJPcGVuZWQiLCAiUmVzb2x2ZWQiXSkKICAgIC53aXRoQ29sdW1uKCJSZXNvbHZlZF9UaW1pbmdfaW5fSG91cnMiLCAodW5peF90aW1lc3RhbXAoIlJlc29sdmVkIikgLSB1bml4X3RpbWVzdGFtcCgiT3BlbmVkIikpIC8gMzYwMCkKICAgIC53aXRoQ29sdW1uKCJTTEFfU3RhdHVzIiwgc2xhX3VkZigiUHJpb3JpdHkiLCAiT3BlbmVkIiwgIlJlc29sdmVkIikpCiAgICAuc2VsZWN0KCJJc3N1ZSIsICJPcGVuZWQiLCAiUmVzb2x2ZWQiLCAiUHJpb3JpdHkiLCAiUmVzb2x2ZWRfVGltaW5nX2luX0hvdXJzIiwgIlNMQV9TdGF0dXMiKQopCgpkaXNwbGF5KGRmX3NsYSkK\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1754205383977,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "be409946-4ecb-47bd-ad7f-e42e0ac1a766",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1754205381256,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1754205381228,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, expr, unix_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 1. Define the SLA classification function\n",
    "def classify_sla(priority, opened, resolved):\n",
    "    if not opened or not resolved:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Convert to datetime if needed\n",
    "    if isinstance(opened, str):\n",
    "        opened = datetime.strptime(opened, \"%Y-%m-%d %H:%M:%S\")\n",
    "    if isinstance(resolved, str):\n",
    "        resolved = datetime.strptime(resolved, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Calculate business hours (excluding weekends)\n",
    "    current = opened\n",
    "    total_hours = 0\n",
    "    while current < resolved:\n",
    "        if current.weekday() < 5:  # Monday to Friday\n",
    "            total_hours += 1\n",
    "        current += timedelta(hours=1)\n",
    "\n",
    "    # Apply SLA thresholds\n",
    "    if priority == \"1-Critical\":\n",
    "        return \"SLA Breached\" if total_hours > 1 else \"SLA Not Breached\"\n",
    "    elif priority == \"2-High\":\n",
    "        return \"SLA Breached\" if total_hours > 6 else \"SLA Not Breached\"\n",
    "    elif priority == \"Medium\":\n",
    "        return \"SLA Breached\" if total_hours > 48 else \"SLA Not Breached\"  # 2 business days\n",
    "    elif priority == \"Low\":\n",
    "        return \"SLA Breached\" if total_hours > 96 else \"SLA Not Breached\"  # 4 business days\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# 2. Register the function as a UDF\n",
    "sla_udf = udf(classify_sla, StringType())\n",
    "\n",
    "# 3. Apply to your DataFrame\n",
    "df_sla = (inc\n",
    "    .withColumn(\"Opened\", expr(\"try_to_timestamp(Opened,'MM/dd/yyyy HH:mm:ss')\"))\n",
    "    .withColumn(\"Resolved\", expr(\"try_to_timestamp(Resolved,'MM/dd/yyyy HH:mm:ss')\"))\n",
    "    .na.drop(subset=[\"Opened\", \"Resolved\"])\n",
    "    .withColumn(\"Resolved_Timing_in_Hours\", (unix_timestamp(\"Resolved\") - unix_timestamp(\"Opened\")) / 3600)\n",
    "    .withColumn(\"SLA_Status\", sla_udf(\"Priority\", \"Opened\", \"Resolved\"))\n",
    "    .select(\"Issue\", \"Opened\", \"Resolved\", \"Priority\", \"Resolved_Timing_in_Hours\", \"SLA_Status\")\n",
    ")\n",
    "\n",
    "display(df_sla)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "002-Logic & Transformation Challenges",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
